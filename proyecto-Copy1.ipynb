{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proyecto Reconocimiento Facial - IIC3724\n",
    "\n",
    "---\n",
    "\n",
    "Felipe Domínguez - 1562188J\n",
    "\n",
    "Sebastián Carreño - 15639746\n",
    "\n",
    "Diego Iruretagoyena - 14619164"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9ymu3p_M0kYe",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from collections import defaultdict\n",
    "\n",
    "from pybalu.io import imread\n",
    "from pybalu.feature_selection import clean, sfs\n",
    "from pybalu.feature_transformation import normalize, pca\n",
    "from pybalu.feature_extraction import haralick_features, lbp_features, gabor_features, hog_features, basic_int_features, fourier_features, gupta_features, flusser_features\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation\n",
    "from keras.optimizers import SGD, Adam\n",
    "from keras.layers import Dense\n",
    "from keras.utils import np_utils\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from tensorflow.keras.callbacks import CSVLogger, Callback\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_PATH = 'FaceMask166'\n",
    "FEATURES_PATH = 'features'\n",
    "NUM_OF_CLASSES = 166"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def open_image(image_path):\n",
    "    image = cv2.imread(f'{DATASET_PATH}/{image_path}')\n",
    "    return image\n",
    "\n",
    "def extract_lbp(image, hdiv, vdiv, grayscale=True, mapping='nri_uniform', channel=None):\n",
    "    \"\"\"\n",
    "    Interface for pybalu implementation\n",
    "    \"\"\"\n",
    "\n",
    "    if grayscale:\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    elif channel != None:\n",
    "        lbp_features(image[:, :, channel], hdiv=hdiv, vdiv=vdiv, mapping=mapping)\n",
    "\n",
    "    return lbp_features(image, hdiv=hdiv, vdiv=vdiv, mapping=mapping)\n",
    "\n",
    "def extract_features(images):\n",
    "    \"\"\"\n",
    "    Returns a hash with all features extracted\n",
    "    \"\"\"\n",
    "\n",
    "    features = defaultdict(list)\n",
    "    \n",
    "    for i, image in enumerate(images):\n",
    "        print(f'Processing image {i}...')\n",
    "\n",
    "        grayscale_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        features['intensity'].append(basic_int_features(grayscale_image))\n",
    "        features['fourier'].append(fourier_features(grayscale_image))\n",
    "        features['haralick'].append(haralick_features(grayscale_image))\n",
    "        features['gabor'].append(gabor_features(grayscale_image))\n",
    "        features['hog_1_1_4'].append(hog_features(grayscale_image, v_windows=1, h_windows=1, n_bins=4))\n",
    "        features['hog_5_5_8'].append(hog_features(grayscale_image, v_windows=5, h_windows=5, n_bins=8))\n",
    "\n",
    "        features['lbp_bw_1_1'].append(extract_lbp(image, 1, 1))\n",
    "        features['lbp_bw_2_2'].append(extract_lbp(image, 2, 2))\n",
    "        features['lbp_bw_4_8'].append(extract_lbp(image, 4, 8))\n",
    "        features['lbp_bw_8_4'].append(extract_lbp(image, 8, 4))\n",
    "\n",
    "        features['lbp_red_8_4'].append(extract_lbp(image, 8, 4, channel=0))\n",
    "        features['lbp_green_8_4'].append(extract_lbp(image, 8, 4, channel=1))\n",
    "        features['lbp_blue_8_4'].append(extract_lbp(image, 8, 4, channel=2))\n",
    "\n",
    "    for key in features.keys():\n",
    "        features[key] = np.array(features[key])\n",
    "\n",
    "    return features\n",
    "\n",
    "def nn_clasiffier(options):\n",
    "    input_dim = options['input_dim']\n",
    "    d1 = options['d1']\n",
    "    d2 = options['d2']\n",
    "    opt = options['opt']\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(d1, input_dim=input_dim, init='uniform', activation='relu'))\n",
    "    model.add(Dense(d2, activation='relu', kernel_initializer='uniform'))\n",
    "    model.add(Dense(NUM_OF_CLASSES))\n",
    "    model.add(Activation('softmax'))\n",
    "\n",
    "    if opt == 'adam':\n",
    "        optimizer = Adam()\n",
    "    elif opt == 'sgd':\n",
    "        optimizer = SGD()\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "def get_classifier(classifier, options=None):\n",
    "    \"\"\"\n",
    "    Returns desired clasiffier\n",
    "    \"\"\"\n",
    "\n",
    "    if classifier == 'knn':\n",
    "        return KNeighborsClassifier(n_neighbors=3)\n",
    "    elif classifier == 'svm':\n",
    "        return SVC(kernel='sigmoid', gamma=0.01, C=0.01)\n",
    "    elif classifier == 'ada':\n",
    "        return AdaBoostClassifier(n_estimators=100)\n",
    "    elif classifier == 'nn':\n",
    "        return nn_clasiffier(options)\n",
    "\n",
    "def normalize_each_class(train_features, val_features, test_features):\n",
    "    \"\"\"\n",
    "    input_features: hash where each key contains a np.array with extracted features\n",
    "\n",
    "    It normalizes each class in train_features, and then normalizes test_features using a,b values from train examples.\n",
    "    \"\"\"\n",
    "    \n",
    "    train_features_norm = {}\n",
    "    val_features_norm = {}\n",
    "    test_features_norm = {}\n",
    "\n",
    "    for key in train_features.keys():\n",
    "        train_norm, a, b = normalize(train_features[key])\n",
    "        val_norm = val_features[key] * a + b\n",
    "        test_norm = test_features[key] * a + b\n",
    "        \n",
    "        train_features_norm[key] = train_norm\n",
    "        val_features_norm[key] = val_norm\n",
    "        test_features_norm[key] = test_norm\n",
    "        \n",
    "    return train_features_norm, val_features_norm, test_features_norm\n",
    "\n",
    "def plot_nn_csv(file_name):\n",
    "    \"\"\"\n",
    "    Plots neural model from csv file saved in callbacks\n",
    "    \"\"\"\n",
    "\n",
    "    file = pd.read_csv(file_name)\n",
    "\n",
    "    (fig, axs) = plt.subplots(2, 1, figsize=(10, 10))\n",
    "\n",
    "    axs[0].plot(file['epoch'], file['loss'])\n",
    "    axs[0].set_title('Loss by epoch')\n",
    "    axs[0].set_xlabel('Epoch #')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "\n",
    "    axs[1].plot(file['epoch'], file['accuracy'])\n",
    "    axs[1].set_title('Train accuracy')\n",
    "    axs[1].set_xlabel('Epoch #')\n",
    "    axs[1].set_ylabel('Accuracy')\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.savefig(f\"{file_name.split('.csv')[0]}.png\")\n",
    "    plt.show()\n",
    "\n",
    "def plot_images(image_1, image_2):\n",
    "    (fig, axs) = plt.subplots(1, 2, figsize=(15, 15))\n",
    "    axs[0].imshow(image_1)\n",
    "    axs[1].imshow(image_2)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "def concat_features(features_hash):\n",
    "    \"\"\"\n",
    "    Return a np.array with all features concatenated\n",
    "    \"\"\"\n",
    "\n",
    "    return np.concatenate(list(features_hash.values()), axis=1)\n",
    "\n",
    "def save_features(features, path):\n",
    "    for key in features.keys():\n",
    "        features_df = pd.DataFrame(data=features[key])\n",
    "        features_df.to_csv(f'{path}/{key}.csv', header=True, index=False)\n",
    "\n",
    "def load_features(path):\n",
    "    features = {}\n",
    "\n",
    "    for file_name in os.listdir(path):\n",
    "        if file_name.endswith('csv'):\n",
    "            feature_name = file_name.split('.')[0]\n",
    "            features[feature_name] = pd.read_csv(f'{path}/{file_name}').to_numpy()\n",
    "\n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para simplificar algunas cosas, el suefo F000166 lo renombramos a F000000 en la base de datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cYceysTCyruT",
    "outputId": "d4b03e86-bccf-4cfe-c404-7a500b19f6ea",
    "tags": []
   },
   "outputs": [],
   "source": [
    "images_names = sorted([file_name for file_name in os.listdir(DATASET_PATH) if file_name.endswith('jpg')])\n",
    "images_names = images_names[:(NUM_OF_CLASSES * 6)]\n",
    "print(images_names[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NqBea5KIzzk5",
    "outputId": "ea00b539-9c2e-469e-8514-374d495505eb",
    "tags": []
   },
   "outputs": [],
   "source": [
    "TRAIN_NAMES = sorted([i for i in images_names if (i.endswith('01.jpg') or i.endswith('02.jpg') or i.endswith('03.jpg'))])\n",
    "VALIDATION_NAMES = sorted([i for i in images_names if i.endswith('04.jpg')])\n",
    "TEST_NAMES = sorted([i for i in images_names if (i.endswith('05.jpg') or i.endswith('06.jpg'))])\n",
    "\n",
    "Y_TRAIN = np.array([int(image_name.split('_')[0][2:]) for image_name in TRAIN_NAMES]).astype(int)\n",
    "Y_VAL = np.array([int(image_name.split('_')[0][2:]) for image_name in VALIDATION_NAMES]).astype(int)\n",
    "Y_TEST = np.array([int(image_name.split('_')[0][2:]) for image_name in TEST_NAMES]).astype(int)\n",
    "\n",
    "print(Y_TRAIN[-1])\n",
    "\n",
    "TRAIN_IMAGES = [open_image(image_path)[:128,:] for image_path in TRAIN_NAMES]\n",
    "VAL_IMAGES = [open_image(image_path)[:128,:] for image_path in VALIDATION_NAMES]\n",
    "TEST_IMAGES = [open_image(image_path)[:128,:] for image_path in TEST_NAMES]\n",
    "\n",
    "def build_dataset(n_clases):\n",
    "    dataset = {}\n",
    "    dataset['n_clases'] = n_clases\n",
    "    dataset['y_train'] = Y_TRAIN[:n_clases]\n",
    "    dataset['y_val'] = Y_VAL[:n_clases]\n",
    "    dataset['y_test'] = Y_TEST[:n_clases]\n",
    "    dataset['train_images'] = TRAIN_IMAGES[:n_clases]\n",
    "    dataset['val_images'] = VAL_IMAGES[:n_clases]\n",
    "    dataset['test_images'] = TEST_IMAGES[:n_clases]\n",
    "    dataset['x_train'] = []\n",
    "    dataset['x_val'] = []\n",
    "    dataset['x_test'] = []\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definir los distintos conjuntos\n",
    "datasets = {}\n",
    "datasets['dataset_16'] = build_dataset(16)\n",
    "datasets['dataset_40'] = build_dataset(40)\n",
    "datasets['dataset_100'] = build_dataset(100)\n",
    "datasets['dataset_166'] = build_dataset(166)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8fojkf-3hocr",
    "outputId": "601e7d5d-6e0f-4cd8-ed11-f24877932183"
   },
   "outputs": [],
   "source": [
    "plot_images(TRAIN_IMAGES[0], TEST_IMAGES[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Vi6NbFE8zzvd",
    "outputId": "6f6aeb8a-b7bd-45c2-ebc0-aa9880d0645c",
    "tags": []
   },
   "outputs": [],
   "source": [
    "LOAD = True\n",
    "\n",
    "if LOAD:\n",
    "    # Option 1: read features from csv files\n",
    "    features_train_hash = load_features(f'{FEATURES_PATH}/train')\n",
    "    features_val_hash = load_features(f'{FEATURES_PATH}/val')\n",
    "    features_test_hash = load_features(f'{FEATURES_PATH}/test')\n",
    "\n",
    "else:\n",
    "    # Option 2: extract features\n",
    "    features_train_hash = extract_features(train_images)\n",
    "    features_val_hash = extract_features(val_images)\n",
    "    features_test_hash = extract_features(test_images)\n",
    "\n",
    "    save_features(features_train_hash, path=f'{FEATURES_PATH}/train')\n",
    "    save_features(features_val_hash, path=f'{FEATURES_PATH}/val')\n",
    "    save_features(features_test_hash, path=f'{FEATURES_PATH}/test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NORMALIZE\n",
    "train_features_norm, val_features_norm, test_features_norm = normalize_each_class(\n",
    "    features_train_hash, \n",
    "    features_val_hash, \n",
    "    features_test_hash\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "features_train_concat = concat_features(train_features_norm)\n",
    "features_val_concat = concat_features(val_features_norm)\n",
    "features_test_concat = concat_features(test_features_norm)\n",
    "\n",
    "print(features_train_concat.shape)\n",
    "print(features_val_concat.shape)\n",
    "print(features_test_concat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# CLEAN\n",
    "clean_indexes = clean(features_train_concat)\n",
    "\n",
    "train_features_cleaned = features_train_concat[:, clean_indexes]\n",
    "val_features_cleaned = features_val_concat[:, clean_indexes]\n",
    "test_features_cleaned = features_test_concat[:, clean_indexes]\n",
    "\n",
    "print(train_features_cleaned.shape)\n",
    "print(val_features_cleaned.shape)\n",
    "print(test_features_cleaned.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_FEATURES_CONCAT = np.nan_to_num(train_features_cleaned).astype(float)\n",
    "VAL_FEATURES_CONCAT = np.nan_to_num(val_features_cleaned).astype(float)\n",
    "TEST_FEATURES_CONCAT = np.nan_to_num(test_features_cleaned).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_features_to_dataset(datasets):\n",
    "    for key in datasets.keys():\n",
    "        datasets[key]['x_train'] = TRAIN_FEATURES_CONCAT[:datasets[key]['n_clases']]\n",
    "        datasets[key]['x_val'] = VAL_FEATURES_CONCAT[:datasets[key]['n_clases']]\n",
    "        datasets[key]['x_test'] = TEST_FEATURES_CONCAT[:datasets[key]['n_clases']]\n",
    "\n",
    "    return datasets\n",
    "\n",
    "datasets = add_features_to_dataset(datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estrategias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cabe destacar que como preprocesamiento de cada estrategia se hace un normalizado y luego un clean. De esta manera se evita repetir código.\n",
    "De todas formas ambos pasos fueron nombrados en cada estrategia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 1: 16 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_16 = datasets['dataset_16']\n",
    "\n",
    "print(DATASET_16['x_train'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA100\n",
    "train_pca_100, _, A, Xm, _ = pca(DATASET_16['x_train'], n_components=100)\n",
    "val_pca_100 = np.matmul(DATASET_16['x_val'] - Xm, A)\n",
    "test_pca_100 = np.matmul(DATASET_16['x_test'] - Xm, A)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SFS\n",
    "sfs_indexes_5 = sfs(DATASET_16['x_train'], DATASET_16['y_train'], n_features=5, method='fisher', show=True)\n",
    "sfs_indexes_10 = sfs(DATASET_16['x_train'], DATASET_16['y_train'], n_features=10, method='fisher', show=True)\n",
    "sfs_indexes_15 = sfs(DATASET_16['x_train'], DATASET_16['y_train'], n_features=15, method='fisher', show=True)\n",
    "sfs_indexes_20 = sfs(DATASET_16['x_train'], DATASET_16['y_train'], n_features=20, method='fisher', show=True)\n",
    "sfs_indexes_25 = sfs(DATASET_16['x_train'], DATASET_16['y_train'], n_features=25, method='fisher', show=True)\n",
    "sfs_indexes_50 = sfs(DATASET_16['x_train'], DATASET_16['y_train'], n_features=50, method='fisher', show=True)\n",
    "sfs_indexes_80 = sfs(DATASET_16['x_train'], DATASET_16['y_train'], n_features=80, method='fisher', show=True)\n",
    "\n",
    "\n",
    "train_sfs_5 = DATASET_16['x_train'][:, sfs_indexes_5]\n",
    "val_sfs_5 = DATASET_16['x_val'][:, sfs_indexes_5]\n",
    "test_sfs_5 = DATASET_16['x_test'][:, sfs_indexes_5]\n",
    "\n",
    "train_sfs_10 = DATASET_16['x_train'][:, sfs_indexes_10]\n",
    "val_sfs_10 = DATASET_16['x_val'][:, sfs_indexes_10]\n",
    "test_sfs_10 = DATASET_16['x_test'][:, sfs_indexes_10]\n",
    "\n",
    "train_sfs_15 = DATASET_16['x_train'][:, sfs_indexes_15]\n",
    "val_sfs_15 = DATASET_16['x_val'][:, sfs_indexes_15]\n",
    "test_sfs_15 = DATASET_16['x_test'][:, sfs_indexes_15]\n",
    "\n",
    "train_sfs_20 = DATASET_16['x_train'][:, sfs_indexes_20]\n",
    "val_sfs_20 = DATASET_16['x_val'][:, sfs_indexes_20]\n",
    "test_sfs_20 = DATASET_16['x_test'][:, sfs_indexes_20]\n",
    "\n",
    "train_sfs_25 = DATASET_16['x_train'][:, sfs_indexes_25]\n",
    "val_sfs_25 = DATASET_16['x_val'][:, sfs_indexes_25]\n",
    "test_sfs_25 = DATASET_16['x_test'][:, sfs_indexes_25]\n",
    "\n",
    "train_sfs_50 = DATASET_16['x_train'][:, sfs_indexes_50]\n",
    "val_sfs_50 = DATASET_16['x_val'][:, sfs_indexes_50]\n",
    "test_sfs_50 = DATASET_16['x_test'][:, sfs_indexes_50]\n",
    "\n",
    "train_sfs_80 = DATASET_16['x_train'][:, sfs_indexes_80]\n",
    "val_sfs_80 = DATASET_16['x_val'][:, sfs_indexes_80]\n",
    "test_sfs_80 = DATASET_16['x_test'][:, sfs_indexes_80]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SFS take to long, let's print this indexes for further loading\n",
    "\n",
    "# s_sfs = [0, 2360, 3699, 710, 3458, 1516, 68, 267, 949, 235, 2446, 3465, 1517, 483, 229]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 1\n",
    "\n",
    "NORM - CLEAN - SFS15 - KNN3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNN 3\n",
    "def estrategia_1(sfs_train, TETS_OR_VAL):\n",
    "    classifier = get_classifier('knn')\n",
    "    classifier.fit(sfs_train, DATASET_16['y_train'])\n",
    "\n",
    "    train_pred = classifier.predict(sfs_train)\n",
    "    val_pred = classifier.predict(TETS_OR_VAL)\n",
    "\n",
    "    train_accuracy = accuracy_score(DATASET_16['y_train'], train_pred) * 100\n",
    "    val_accuracy = accuracy_score(DATASET_16['y_val'], val_pred) * 100\n",
    "\n",
    "    conf_matrix = confusion_matrix(DATASET_16['y_val'], val_pred)\n",
    "\n",
    "    print(f'\\nTrain Accuracy: {train_accuracy}')\n",
    "    print(f'Val Accuracy: {val_accuracy}\\n')\n",
    "\n",
    "    print(conf_matrix)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST SFS 5 ----------------------\")\n",
    "estrategia_1(train_sfs_5,  test_sfs_5)\n",
    "print(\"VALIDACION SFS 5 ----------------------\")\n",
    "estrategia_1(train_sfs_5,  val_sfs_5)\n",
    "print(\"TEST SFS 10 ----------------------\")\n",
    "estrategia_1(train_sfs_10,  test_sfs_10)\n",
    "print(\"VALIDACION SFS 10 ----------------------\")\n",
    "estrategia_1(train_sfs_10,  val_sfs_10)\n",
    "print(\"TEST SFS 15 ----------------------\")\n",
    "estrategia_1(train_sfs_15,  test_sfs_15)\n",
    "print(\"VALIDACION SFS 15 ----------------------\")\n",
    "estrategia_1(train_sfs_15,  val_sfs_15)\n",
    "print(\"TEST SFS 20 ----------------------\")\n",
    "estrategia_1(train_sfs_20,  test_sfs_20)\n",
    "print(\"VALIDACION SFS 20 ----------------------\")\n",
    "estrategia_1(train_sfs_20,  val_sfs_20)\n",
    "print(\"TEST SFS 25 ----------------------\")\n",
    "estrategia_1(train_sfs_25,  test_sfs_25)\n",
    "print(\"VALIDACION SFS 25 ----------------------\")\n",
    "estrategia_1(train_sfs_25,  val_sfs_25)\n",
    "print(\"TEST SFS 50 ----------------------\")\n",
    "estrategia_1(train_sfs_50,  test_sfs_50)\n",
    "print(\"VALIDACION SFS 50 ----------------------\")\n",
    "estrategia_1(train_sfs_50,  val_sfs_50)\n",
    "print(\"TEST SFS 80 ----------------------\")\n",
    "estrategia_1(train_sfs_80,  test_sfs_80)\n",
    "print(\"VALIDACION SFS 80 ----------------------\")\n",
    "estrategia_1(train_sfs_80,  val_sfs_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 2\n",
    "\n",
    "NORM - CLEAN - SFS15 - ADA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADA\n",
    "\n",
    "def estrategia_2(TRAIN, VAL_TEST):\n",
    "    classifier = get_classifier('ada')\n",
    "    classifier.fit(TRAIN, DATASET_16['y_train'])\n",
    "\n",
    "    train_pred = classifier.predict(TRAIN)\n",
    "    val_pred = classifier.predict(VAL_TEST)\n",
    "\n",
    "    train_accuracy = accuracy_score(DATASET_16['y_train'], train_pred) * 100\n",
    "    val_accuracy = accuracy_score(DATASET_16['y_test'], val_pred) * 100\n",
    "\n",
    "    conf_matrix = confusion_matrix(DATASET_16['y_test'], val_pred)\n",
    "\n",
    "    print(f'\\nTrain Accuracy: {train_accuracy}')\n",
    "    print(f'Val Accuracy: {val_accuracy}\\n')\n",
    "\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"TEST SFS 5 ----------------------\")\n",
    "estrategia_2(train_sfs_5,  test_sfs_5)\n",
    "print(\"VALIDACION SFS 5 ----------------------\")\n",
    "estrategia_2(train_sfs_5,  val_sfs_5)\n",
    "print(\"TEST SFS 10 ----------------------\")\n",
    "estrategia_2(train_sfs_10,  test_sfs_10)\n",
    "print(\"VALIDACION SFS 10 ----------------------\")\n",
    "estrategia_2(train_sfs_10,  val_sfs_10)\n",
    "print(\"TEST SFS 15 ----------------------\")\n",
    "estrategia_2(train_sfs_15,  test_sfs_15)\n",
    "print(\"VALIDACION SFS 15 ----------------------\")\n",
    "estrategia_2(train_sfs_15,  val_sfs_15)\n",
    "print(\"TEST SFS 20 ----------------------\")\n",
    "estrategia_2(train_sfs_20,  test_sfs_20)\n",
    "print(\"VALIDACION SFS 20 ----------------------\")\n",
    "estrategia_2(train_sfs_20,  val_sfs_20)\n",
    "print(\"TEST SFS 25 ----------------------\")\n",
    "estrategia_2(train_sfs_25,  test_sfs_25)\n",
    "print(\"VALIDACION SFS 25 ----------------------\")\n",
    "estrategia_2(train_sfs_25,  val_sfs_25)\n",
    "print(\"TEST SFS 50 ----------------------\")\n",
    "estrategia_2(train_sfs_50,  test_sfs_50)\n",
    "print(\"VALIDACION SFS 50 ----------------------\")\n",
    "estrategia_2(train_sfs_50,  val_sfs_50)\n",
    "print(\"TEST SFS 80 ----------------------\")\n",
    "estrategia_2(train_sfs_80,  test_sfs_80)\n",
    "print(\"VALIDACION SFS 80 ----------------------\")\n",
    "estrategia_2(train_sfs_80,  val_sfs_80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 3\n",
    "\n",
    "NORM - CLEAN - PCA - KNN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 4\n",
    "\n",
    "NORM - CLEAN - PCA - NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 5\n",
    "\n",
    "NORM - CLEAN - PCA - SFS15 - KNN3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estrategia 6\n",
    "\n",
    "NORM - CLEAN - PCA - SFS15 - NN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 2: 40 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_40 = datasets['dataset_40']\n",
    "\n",
    "print(DATASET_40['x_train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 3: 100 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_100 = datasets['dataset_100']\n",
    "\n",
    "print(DATASET_100['x_train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset 4: 166 clases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "DATASET_166 = datasets['dataset_166']\n",
    "\n",
    "print(DATASET_166['x_train'].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BASE PARA PROBAR:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choose features to evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "itPI6wvihods",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# KNN 3\n",
    "classifier = get_classifier('knn')\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "train_pred = classifier.predict(x_train)\n",
    "test_pred = classifier.predict(x_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_pred) * 100\n",
    "test_accuracy = accuracy_score(y_test, test_pred) * 100\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(f'\\nTrain Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}\\n')\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# SVM\n",
    "classifier = get_classifier('svm')\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "train_pred = classifier.predict(x_train)\n",
    "test_pred = classifier.predict(x_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_pred) * 100\n",
    "test_accuracy = accuracy_score(y_test, test_pred) * 100\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(f'\\nTrain Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}\\n')\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ADA\n",
    "classifier = get_classifier('ada')\n",
    "classifier.fit(x_train, y_train)\n",
    "\n",
    "train_pred = classifier.predict(x_train)\n",
    "test_pred = classifier.predict(x_test)\n",
    "\n",
    "train_accuracy = accuracy_score(y_train, train_pred) * 100\n",
    "test_accuracy = accuracy_score(y_test, test_pred) * 100\n",
    "\n",
    "conf_matrix = confusion_matrix(y_test, test_pred)\n",
    "\n",
    "print(f'\\nTrain Accuracy: {train_accuracy}')\n",
    "print(f'Test Accuracy: {test_accuracy}\\n')\n",
    "\n",
    "print(conf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# NN with 2 hidden layers\n",
    "feat_dimension = x_train[0].shape[0]\n",
    "model = get_classifier('nn', options={'input_dim': feat_dimension, 'd1': 10, 'd2': 10, 'opt': 'adam'})\n",
    "\n",
    "# Labels to one-hot encoding\n",
    "categorical_train = np_utils.to_categorical(y_train, num_classes=NUM_OF_CLASSES)\n",
    "categorical_test = np_utils.to_categorical(y_test, num_classes=NUM_OF_CLASSES)\n",
    "\n",
    "# NN callbacks\n",
    "csv_logger = CSVLogger('plots/nn_1.csv', separator=',', append=False)\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=2)\n",
    "callbacks = [csv_logger, early_stop]\n",
    "\n",
    "start = datetime.now()\n",
    "\n",
    "# TODO: use validation set to keep best model\n",
    "model.fit(x_train, categorical_train, epochs=100, batch_size=4, verbose=1, callbacks=callbacks)\n",
    "\n",
    "run_time = datetime.now() - start\n",
    "print('Train time: ', run_time)\n",
    "\n",
    "(loss, accuracy) = model.evaluate(x_test, categorical_test, batch_size=4, verbose=1)\n",
    "\n",
    "\n",
    "print('Test Accuracy: ', accuracy * 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nn_csv('plots/nn_1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "proyecto.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
